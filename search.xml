<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Scrapy]]></title>
    <url>%2F2019%2F03%2F21%2FScrapy%2F</url>
    <content type="text"><![CDATA[Scrapy 纯Python实现 开源爬虫框架1家独大 通过Pipeline和Middleware 可实现定制化以和扩展爬虫功能 天生异步，底层基于twisted实现，无需考虑并发问题 工作流程及组件组件 Scrapy Engine: Scrapy内核，负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等 Scheduler: 负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎发起请求时，讲Request交给引擎 Downloader 负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理 Spider 需要后期配置最多的部分，负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器) Item Pipeline 负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方 Downloader Middlewares 可以自定义扩展下载功能的组件 Spider Middlewares 可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests 运行流程 Spiders 将需要爬取得url传递 Engine，随后Engine将其存储在Scheduler中，排队入列 Schedulers讲处理好的request返回给 Engine，Engine将request传递给Downloader，使其按照下载中间件的设置进行下载 Downloader 会将下载好的responses返还给Engine，如果下载失败，失败的request会经过Engine返回Schedulers，重新排队入列，等待下次分配 Engine会将返回的responses交给Spider，此时默认交给def parse() 函数进行处理 Spider会抽取所需要的信息，分为Item数据和需要进一步爬取的url，Item数据通过Engine传递给Item pipeline，url则再返回scheduler中重新入列排队 Item pipeline中存储着最终我们想要的数据，而Scheduler收到更新的url会重复执行第二部，从而一直循环下去，直到项目结束。 项目实战创建​ 此处以https://hr.tencent.com/position.php校招作为尝试，静态页面，基本不设置反爬措施， 1scrapy startproject tenCent #创建一个scrapy项目,tenCent为项目名 创建后会生成如下目录 12345678910tenCent/ tenCent/ # 项目的Python模块，导入自己代码的话需要从这里导入 spiders/ # 一个将会存放你的爬虫的目录 __init__.py __init__.py items.py # 配置Item的文件 middlewares.py # 配置中间件的文件 pipelines.py # 配置管道的文件 settings.py # 项目的配置文件 scrapy.cfg # 部署的配置文件 项目创建好后，转至spiders目录下，创建爬虫文件 123#tencent为爬虫的文件名#tencent.com 为制定爬取的域名，设置好后会过滤掉其他的网站scrapy genspider tencent tencent.com 编辑Items​ 明确爬取的目标，即爬取的数据形式,编写items.py文件 ，计划爬取腾讯招聘的职位名称、类型、人数、工作地点、发布时间、职位详细链接以及详细的工作内容 12345678910111213import scrapyclass TencentItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() position_name = scrapy.Field() position_type = scrapy.Field() people_number = scrapy.Field() work_location = scrapy.Field() publish_times = scrapy.Field() position_link = scrapy.Field() detailContent = scrapy.Field() Spiders明确爬取目标后，即开始编辑爬取的代码，进行目标网站分析 腾讯招聘的网页结构并不复杂，单个职位的信息都包含在一个tr标签中，一页中10个职位都包含在tbody标签中，路径清晰，利用xpath实现对页面数据的提取 下一页链接如图所示，获取后拼接url实现翻页功能 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding: utf-8 -*-import scrapy#开始是需要从刚才编写的管道中导入所需的类from tenCent.items import TencentItemclass TencentSpider(scrapy.Spider): name = 'tencent' #爬虫名，创建时自带 allowed_domains = ['tencent.com'] #允许爬取的域名，创建时自带 start_urls = ['https://hr.tencent.com/position.php'] #爬取的初始url，自行设置 base_url = 'https://hr.tencent.com/' #用于后面进行拼接翻页功能 def parse(self, response): #提取单个页面中所有职位的标签 node_list = response.xpath('//tr[@class="even"] | //tr[@class="odd"]') next_link = response.xpath('//a[@id="next"]/@href').extract_first() #遍历后，对每个职位信息进行分别信息抽取 #此处需要实例化管道，用于接收分别提取的信息 #extract_first()防止提取的信息为空而报错，会返回None for node in node_list: item = TencentItem() item['position_name'] = node.xpath('./td/a/text()').extract_first() item['position_link'] = node.xpath('./td/a/@href').extract_first() item['position_type'] = node.xpath('./td[2]/text()').extract_first() item['people_number'] = node.xpath('./td[3]/text()').extract_first() item['work_location'] = node.xpath('./td[4]/text()').extract_first() item['publish_times'] = node.xpath('./td[5]/text()').extract_first() detail_page = self.base_url + item['position_link'] #meta 用于传递数据，此时只收取了6个数据，还有详情页的数据没有收集，meta将函数的数据收集并传递到下一个函数中 #meta必须接受一个字典 yield scrapy.Request(url=detail_page, callback=self.detail_parse, meta=&#123;"item":item&#125;) #拼接下一页的链接，回调递归给parse函数，进一步解析 next_url = self.base_url + next_link yield scrapy.Request(url=next_url, callback=self.parse) def detail_parse(self, response): item = response.meta['item'] #将提取的每条职位详细要求进行拼接，形成一个完整的字段 item['detailContent'] = "".join(response.xpath('//ul[@class="squareli"]/li/text()').extract_first()) yield item Pipelinespipelines的格式基本固定，只需做简单调整即可 1234567891011121314151617import jsonclass TencentPipeline(object): def open_spider(self, spider): #文件以tencent.json格式写入文件 self.file = open("tencent.json", "w") def process_item(self, item, spider): content = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(content) return item def close_spider(self, spider): self.file.close() 设置好pipelines后，需要在settings中注册管道，即打开数据传输通道。为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高） 运行配置完后，启动爬虫 1scrapy crawl tencent 运行截图如下 获取的tencent.json文件如下： 优化Crawlspiders通过获取下一页链接，实现翻页功能并不能完全发挥scrapy异步的特性，采用一个新的方法去加速网站的爬取。 在spiders目录下创建crawlspiders 1scrapy genspider -t crawl tencent tencent.com crawlspiders继承了Spider类中的 name、allow_domains等属性，并提供了新的方法如： Rules: 12345678class scrapy.spiders.Rule( link_extractor, #定义需要提取的链接，通过正则匹配 callback = None, #制定提取链接的回调函数 cb_kwargs = None, follow = None, #判定从response提取的里链接是否需要跟进 process_links = None, #指定spiders中哪个函数会被调用，主要用来过滤 process_request = None ) 1234567891011121314151617181920212223242526272829303132333435363738import scrapy#导入LinkExtractor类from scrapy.linkextractors import LinkExtractor#导入crawlspider，rule类from scrapy.spiders import CrawlSpider, Rulefrom tenCent.items import TencentItemclass TencentSpider(CrawlSpider): name = 'tencent' allowed_domains = ['tencent.com'] start_urls = ['https://hr.tencent.com/position.php'] base_url = 'https://hr.tencent.com/' rules = ( #提取符合规则的url；请求返回函数为parse_item，并跟进链接，response传下去继续匹配 # callback 千万不能写 parse，再次强调：由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。 Rule(LinkExtractor(allow=r'start=\d+'), callback='parse_item', follow=True) ) def parse_item(self, response): node_list = response.xpath('//tr[@class="even"] | //tr[@class="odd"]') for node in node_list: item = TencentItem() item['position_name'] = node.xpath('./td/a/text()').extract_first() item['position_link'] = node.xpath('./td/a/@href').extract_first() item['position_type'] = node.xpath('./td[2]/text()').extract_first() item['people_number'] = node.xpath('./td[3]/text()').extract_first() item['work_location'] = node.xpath('./td[4]/text()').extract_first() item['publish_times'] = node.xpath('./td[5]/text()').extract_first() detail_page = self.base_url + item['position_link'] yield scrapy.Request(url=detail_page, callback=self.detail_parse, meta=&#123;"item":item&#125;) def detail_parse(self, response): item = response.meta['item'] item['detailContent'] = "".join(response.xpath('//ul[@class="squareli"]/li/text()').extract_first()) yield item 数据分开存储第二部分我们将爬取到的职位信息和具体内容存储到一个文件中，有时我们需要将数据进行分开存储，我们可以通过定义两个items类来接收不同的数据 Items12345678910111213141516import scrapyclass TencentItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() position_name = scrapy.Field() position_type = scrapy.Field() people_number = scrapy.Field() work_location = scrapy.Field() publish_times = scrapy.Field() position_link = scrapy.Field()#定义一个专门用于接收详情页工作内容的管道class DetailItem(scrapy.Item): detailContent = scrapy.Field() Pipelines123456789101112131415161718192021222324252627282930313233343536import json#导入Item中定义的两个类from tenCent.items import TencentItem, DetailItemclass TencentPipeline(object): def open_spider(self, spider): self.file = open("tencent.json", "w") def process_item(self, item, spider): #使用isinstance判断是否是该管道需要接收的数据，如果是则存储，如果不是则转到下一个类中进一步判断 if isinstance(item, TencentItem): content = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(content) return item def close_spider(self, spider): self.file.close()class DetailPipeline(object): def open_spider(self, spider): self.file = open("detail.json", "w") def process_item(self, item, spider): if not isinstance(item, TencentItem): content = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(content) return item def close_spider(self, spider): self.file.close() 最后会在spiders中分别形成两个存储文件 Logging运行爬虫后，scrapy会给出很多debug信息，会影响对所获取数据的判断，通过调试logging等级，可以精简信息 log level分为五个等级 CRITICAL - 严重错误(critical) ERROR - 一般错误(regular errors) WARNING - 警告信息(warning messages) INFO - 一般信息(informational messages) DEBUG - 调试信息(debugging messages) 可以在settings中添加两行文件，会得到较为清爽的数据页面 12LOG_FILE = "TencentSpider.log" #将日志写入tencentspider.log中LOG_LEVEL = "INFO" #设置log显示和记录的等级]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[selenium]]></title>
    <url>%2F2019%2F03%2F01%2Fselenium%2F</url>
    <content type="text"><![CDATA[Selenium有了selenium，你可以指挥浏览器做任何事 相识简介​ Selenium是一个用于web应用测试工具，其在浏览器中测试运行，就像真正的用户在实际操作，支持IE，Chrome，Firefox等。 安装​ 使用selenium操作浏览器需要配置对应的浏览器，这里以chrome为例。 chromedriver版本需要与chrome版本对应，从浏览器查看版本，选择对应的版本即可 mac解压后，放入/usr/local/bin 目录中 win使用时，可以直接拖入python/scripts/目录下，或者一个配置了环境变量的问价中 安装即用pin install selenium即可 相知Chrome Options可以将chrome理解成一个对象，通过add_argument()方法来添加相关的参数 12345678910111213141516171819202122232425262728#导入浏览器驱动模块from selenium import webdriver#导入驱动的参数配置from selenium webdriver.chrome.options import Options#实例化一个启动参数对象chrome_options = Options()#设置浏览器以无界面方式运行chrome_options.add_argument('--headless')#官方文档说明上表示此操作是为了规避bug，但是在之后的版本会撤销此操作chrome_options.add_argument('--disable-gpu')#设置浏览器窗口大小，有些网页在大小不同时，有的模块会无法加载出来chrome_options.add_argument('-window-size=1366,768')#禁用加载prefs=&#123; 'profile.default_content_setting_values':&#123; 'images': 2 #禁用图片加载 'notifications': 2 #禁用浏览器弹窗 &#125;&#125;#设置好参数后，启动浏览器browser = webdriver.Chrome(chrome_options=chrome_options) 常用的启动参数（全部参数） 启动参数 作用 –user-agent=“” 设置请求头的User-Agent –window-size=1366,768 设置浏览器窗口大小 –headless 无界面运行 –start-maximized 最大化运行 –incognito 隐身模式 –disable-javascript 禁用JavaScript –disable-infobars 禁用浏览器正在被自动化程序控制的提示 Chrome webdriver指定位置有时为了项目部署和打包，除了将chormedriver.exe部署在环境变量中，还可指定传入的路径 12from selenium import webdriverbrowser = webdriver.Chrome(executable_path='chromedriver.exe') 常用方法1234567891011121314151617181920212223#传入urldriver.get(url)#关闭浏览器当前的窗口driver.close()#退出webdriver并关闭所有窗口driver.quit()#刷新当前页面driver.refresh()#获取当前页面的标题driver.title()#获取当前页面渲染后的源代码！！driver.page_source()#获取当前页面的urldriver.current_url()#获取当前会话中所有窗口的句柄，返回的是一个列表driver.window_handles() 查找元素固定查找 方法 作用 browser.find_element_by_class_name() 通过Class属性查找 browser.find_element_by_xpath() 通过Xpath查找 browser.find_element_by_css_selector() 通过CSS选择器查找 browser.find_element_by_id() 通过id查找 browser.find_element_by_partial_link_text() 通过链接文本的部分匹配查找 browser.find_element_by_name() 通过name属性进行查找 browser.find_element_by_link_text() 通过链接文本查找 browser.find_element_by_tag_name() 通过标签名查找 ==所有查找返回的是一个webelement对象。== 以上方法返回的都是单个对象，如果想多个匹配对象，只需将 element换成elements即可。 定制化查找除了以上的已经固定的查找方式，还有两种私有方法find_element()和find_elements()可以使用。 123456789101112131415#By这个类是专门用来查找元素时传入的参数，这个类中有以下属性ID = "id"XPATH = "xpath"LINK_TEXT = "link text"PARTIAL_LINK_TEXT = "partial link text"NAME = "name"TAG_NAME = "tag name"CLASS_NAME = "class name"CSS_SELECTOR = "css selector"from selenium.webdriver.common.by import Bydriver.find_element(By.XPATH, '//button[text()="Some text"]')driver.find_elements(By.XPATH, '//button') Cookie给当前会话添加cookie。add_cookie(cookie_dict) 12345678910111213141516"""cookie_dict: 一个字典对象，必须要有'name'和'value'两个键，可选的键有：'path', 'domain', 'secure', 'expiry'"""#按name获取大哥cookie，没有则返回Noneget_cookie(name)#获取所有cookie，返回的是一组字典get_cookies(name)#删除所有cookiesdelete_all_cookies()#按name删除指定cookiedelete_cookie(name) 切换窗口和框架 Switch_to.frame() ​ 在页面中如果存在iframe这样的页面子框架的话，selenium是无法搜索到子框架frame中的元素并定位的，所以需要切换到frame中。 123456#先采用搜索方法定位到iframe框架，然后切入到frame中frame = driver.find_element_by_tag_name('iframe')driver.switch_to.frame(frame)#再切换回主界面driver.switch_to.default_content() Switch_to.window(window_name) 可以在一个浏览器中的窗口中互相切换，此方法需要传入目标窗口的句柄，窗口句柄可以通过driver.window_handlesdeng 等方法进行获取 123windows = driver.window_handles#切换到最新打开的窗口中switch_to.window(windows[-1]) 获取截屏 Get_screenshot_as_base64() 获取当前窗口的截图保存为一个base64编码的字符串 Get_screenshot_as_file(filename) 讲获取的截屏保存为一个png格式的图片，filename参数为图片的保存地址，最后应该以.png结尾。如果出现IO错误，则返回Fasle 1driver.get_screenshot_as_file('/desktop/截屏.png') get_screenshot_as_png() 获取当前窗口的截图保存为一个png格式的二进制字符串 获取窗口信息 get_window_position(windowHandle=’current’) 获取当前窗口的x, y坐标 get_window_rect() 获取当前窗口的x, y 坐标和当前窗口的高度和宽度 get_window_siza(windowHandle=’current’) 获取当前窗口的高度和宽度 执行JS代码 execute_async_script(script, *args) 在当前的window/frame 中异步执行JS代码 script：要执行的代码 *args: JS代码执行要传入的参数 execute_script(script, *args) 在当前的window/frame中同步执行JS代码 script：要执行的代码 *args: JS代码执行要传入的参数 Webelement在通过find方法找到页面元素对象时，有时要进行进一模拟操作 方法 方法 效果 clear（） 清空当前文本 click（） 点击当前元素 get_attribute(name) 获取元素的attribute/property，优先返回完全匹配属性名的值，如果不存在，则返回属性名中包含的name值 screenshot(filename) 获取当前元素的截图，保存为png，此方法在chrome中无法使用 send_keys（） 模拟键入元素 submit（） 提交表单 属性 属性 效果 text 获取当前元素的文本内容 tag_name 获取当前元素的标签名 size 获取当前元素的大小 screenshot_as_png 将当前元素截屏保存为png格式的二进制 screenshot_as_base64 将当前元素截屏保存为base64编码的字符串 rect 获取一个包含当前元素大小和位置的字典 parent 获取当前元素的父节点 location 当前元素的位置 keys 当需要进行模拟键盘等复杂操作时，在send_keys()方法中传入要输入的字符串即可 12345from selenium.webdriver.common.keys import Keyselem.send_keys(Keys.RETURN)#如果需要进行组合操作，以此传入对应的键值即可elem.send_keys(Keys.RETURN，'a') keys的类有很多属性，具体的对应，可具体查看。 Actions Chains操作动作链动作链可以让selenium完成更复杂的一些操作，例如拖动，双击，长按。 123456789101112131415#导入模块from selenium.webdriver import ActionChains#设置传入参数element = driver.find_element_by_name("source")target = driver.find_element_by_name("target")#声明一个动作链对象，并将对象赋值给一个actionsactions = ActionChains(driver)#调用actions其内部附带的各种动作方法进行操作actions.drag_and_drop(element, target)#在调用各种动作方法后，不会立即执行，按顺序存储在ActionChains队列中，当调用perform时，这些动作才会依次开始actions.perform() 常用的动作方法 click(on_element=None) 左键单击传入的元素，如果不传入的话，点击鼠标当前位置。 context_click(on_element=None) 右键单击。 double_click(on_element=None) 双击。 click_and_hold(on_element=None) 点击并抓起 drag_and_drop(source, target) 在source元素上点击抓起，移动到target元素上松开放下。 drag_and_drop_by_offset(source, xoffset, yoffset) 在source元素上点击抓起，移动到相对于source元素偏移xoffset和yoffset的坐标位置放下。 send_keys(*keys_to_send) 将键发送到当前聚焦的元素。 send_keys_to_element(element, *keys_to_send) 将键发送到指定的元素。 reset_actions() 清除已经存储的动作 等待​ 每次请求url，selenium会等页面加载完毕才会再执行程序，但是由于ajax和JS代码异步加载的问题，我们无法定位尚未加载出来的元素，这时就会引发报错，对此，我们通过等待来解决。 显示等待123456789101112131415from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome()driver.get("http://somedomain/url_that_delays_loading")#try语句表示，在无法加载元素抛出异常时，最多等待10s，在这10s中，webdriver会默认以0.5s运行一次until中的内容try: element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.ID, "myDynamicElement")) )finally: driver.quit() 更多的等待操作，可查看官方文档 隐式等待隐式等待指的是，在webdriver中进行find_element这一类查找操作时，如果找不到元素，则会默认的轮询等待一段时间。 123456from selenium import webdriverdriver = webdriver.Chrome()driver.implicitly_wait(10) # 单位是秒，默认时间为0，可根据情况设置driver.get("http://somedomain/url_that_delays_loading")myDynamicElement = driver.find_element_by_id("myDynamicElement") 相弃页面滚动有些页面为了性能的考虑，页面下方不在当前屏幕中的元素是不会加载的，只有当页面向下滚动时才会继续加载。 而selenium本身不提供向下滚动的方法，所以我们需要去用JS去滚动页面： 1driver.excute_script("window.scrollTo(0, document.body.scrollHeight)") 元素遮挡​ 有时候因为一些弹出元素的原因，用EC.presence_of_element_located()的话，我们需要定位的元素就无法被找到，这个时候我们就应该使用EC.visibility_of_element_located()方法可以在等待到当前元素可见后，才获取元素。 123element = WebDriverWait(driver, 10).until( EC.visibility_of_element_located((By.XPATH, ''))) 分辨率无法匹配由于分辨率设置的原因，查找的元素当前是不可见的，这时候应该设置好分辨率，使当前元素能够显示到页面中 Xpath无法直接获取元素属性Selenium同样不支持Xpath中的string()，text()这类的方法，只能获取元素节点，再通过get_attribute(name)方法来获取属性 123page.xpath('//div/a/@class')page.xpath('//div/a').get_attribute('class')]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法]]></title>
    <url>%2F2019%2F02%2F02%2F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据结构与算法基础数据结构 — 一组数据的存储结构 算法 — 操作数据的一组方法 数据结构是为算法服务的，算法要作用在特定的数据结构之上 时间、复杂度如何评价一个程序运行的效率： 最能先想到的应该是时间，以一个程序开始到结束运行的时间。 1.测试结果非常依赖测试环境，软件版本，电脑本身的硬件设施 2.测试结果受到数据规模的影响，两个程序在运行少量数据时效率无明显差别，一但上升到千万级亿万级呢？ 大（O）复杂度表示法： 以程序运行一次记作一步，一步指的是需要固定时间量的操作，比如将变量绑定到对象、做一次比较、执行一次代数运算或者访问内存中的对象。即复杂度 分为： ​ 最佳情形：运行时间是输入最有利的情况下算法的运行时间 ​ 最差情形：运行时间是在给定输入规模的情况下最长的运行时间 ​ 平均情形：运行时间是在给定输入规模的情况下的平均运行时间，此外，对于 输入值的分布有一些先验信息 渐近表示法： ​ 如果运行时间是一个多项式的和，那么保留增长速度最快的项，去掉其他各项；如果剩下的项是个乘积，那么去掉所有常数 常用的即大O表示法，其可以表示给出一个函数渐近增长的上限。 列举一些最常用的大（O）表示法，n表示函数的输入规模。 O(1) 表示常数运行时间 O(logn)表示对数运行时间 O(n)表示线性运行时间 O(n*log(n)表示对数线性运行时间 O(nk)表示多项式运行时间，k是常数 O（cn）表示指数运行时间，常数c为底数，复杂度为c的n次方 搜索算法简单查找1234567#搜索L中是否有edef search(L, e) for i in range(len(L)): if L[i] == e: return True else: return False L为无序列表，此时最坏的情况O（len（L）），我们需要遍历L中所有的元素才能找到e，或者e不存在L中。 二分查找 思路： 选择一个可以将列表元素一分为二的索引i 检查是否L[i] == e 如果不是，检查L[i]大于还是小于e 根据3的结果，确定接下来是在L的左半部分还是右半部分搜索e 123456789101112131415161718192021222324252627def midsearch(L, e): """ 假设L是列表，其中元素按升序排列。 ascending order. 如果e是L中的元素， 则返回True，否则返回False """ def midsearch(L, e, low, high): #递归结束条件 if low == high: return L[low] == e mid = (low + high)//2 if L[mid] == e: return True elif L[mid] &gt; e: if low == mid: return False else: return midsearch(L, e, low, mid-1) else: return midsearch(L, e, mid+1, high) if len(L) == 0: return False else: return midsearch(L, e, 0, len(L)-1) ​ 二分法查找的前提是L为有序集合，无序会造成混乱，二分查找每次判断的过程其实只有midsearch这一个步骤，当不断半数切割时，重复调用midsearch本身这个函数，执行的次数为log(len(L))，所以二分的复杂度其实就是O（log(len(L))）。 ​ 二分查找的前提是有序集合，所以相对集合进行搜索需要对列表进行排序，但任何列表排序的时间都会大于O(len(L))，而完成搜索所需要的就变成了O(len(L))+O（log(len(L))），这样明显不划算，但如果只完成一次排序，可以用于k次搜索，这样就变成额O(len(L))+kO（log(len(L))），此时排序所需的时间就会被忽略。 排序选择排序12345678def selSort(L): start = 0 while(start != len(L)): for i in range(start, len(L)): if L[i] &lt; L[start]: L[start], L[i] = L[i], L[start] start += 1 return L 内层循环的复杂度为𝑂(𝑙𝑒𝑛(𝐿))O(len(L))，外层循环的复杂度也是𝑂(𝑙𝑒𝑛(𝐿))O(len(L))。所以，整个函数的复杂度是𝑂(𝑙𝑒𝑛(𝐿)2)O(len(L)2)，即列表L长度的平方。 归并排序归并的思想采用的是分而治之，即把问题分为若干个小问题，取其中最优的解再合并作为最初问题的答案。由约翰·冯·诺依曼发明。 思路： 如果列表的长度是0或1，那么它已经排好序了； 如果列表包含多于1个元素，就将其分成两个列表，并分别使用归并排序法进行排序； 合并结果，合并的思想是先看每个列表的第一个元素，然后讲两者之间较小的一个移到目标列表的末尾，其中一个列表为空时，就将另一个列表中月下的元素复制到目标列表末 1234567891011121314151617181920212223242526def merge(left, right): result = [] i, j = 0, 0 while i &lt; len(left) and j &lt; len(right): if left[i] &lt; right[j] result.append(left[i]) i += 1 else: result.append(right[i]) j += 1 while i &lt; len(left): result.append(right[j]) i += 1 while j &lt; len(right): result.append(left[i]) j += 1 return resultdef mergeSort(L): if len(L) &lt; 2: return L else: middle = len(L)//2 left = mergeSort(L[:middle]) right = mergeSort(L[middle:]) return merge(left, right) merge函数中每次执行的步骤是Len（left+right=L)，mergeSort函数中，每次递归调用O(log(len(L))),所以归并排序的复杂度是，O(len(L))*O(log(len(L))). 快速排序选定一个基准值（pivot），将被排序的数组分区，左边为小于基准值的子数组，中间为基准值，右边为大于基准值的子数组，再分别对子数组进行快速排序，递归往下，直至只有空数组或者只包含一个元素的数组的的基线条件。 123456789101112131415161718192021222324252627282930def QuickSort(L, start, end): #建立递归终止条件 if start &gt;= end: return #low为左边序列要移动的游标 low = start #last为右边序列要移动的游标 last = end #将起始元素设为要寻找位置的基准元素 mid_num = L[start] while low &lt; last: #如果last和low未重合，且比基准元素要大，游标向左移动 while low &lt; last and L[last] &gt;= mid_num: last -= 1 # 如果比基准元素小，就跳出循环，并且把其放在基准元素左边 L[low] = L[last] #如果last和low未重合，且比基准元素要小，游标向右移动 while low &lt; last and L[low] &lt;= mid_num: low += 1 # 如果比基准元素大，就跳出循环，并且把其放在基准元素右边 L[last] = L[low] #当low与last相等，就是mid_num的排序位置 L[low] = mid_num #分别对左右子集合进行递归 QuickSort(L, start, last-1) QuickSort(L, low+1, end) ​ 由此可以看出，对于一个未知的序列，快速排序的性能高度依赖你选取的基准元素，如果我们总是将第一个元素作为基准值，那么最糟糕的情况即是对一个有序数组进行快速排序，这样每层栈调用的就是O（n），一共调用了n层栈，即最坏的情况O（n2）, 最好的情况即使取中间值，一共调用了log（n）层栈，即最佳情况为O(n log n)。此处的最佳情况也是平均情况，如果随机选择一个数组元素作为基准值，那么其运行时间就是最佳时间。 ​ 关于快速排序和归并排序的比较： 两者的最佳运行时间都为O（n log n），那么如何取舍？，快速查找的常量比合并查找时要小，因为即使运行时间相同，但快速查找速度要更快，且应用更多。常量有时候可以被省略，比如简单查找和二分查找，当数据规模大时几乎可以被忽略。但是对于快速排序和归并排序不可省略。]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[代理池]]></title>
    <url>%2F2019%2F01%2F26%2F%E4%BB%A3%E7%90%86%E6%B1%A0%2F</url>
    <content type="text"></content>
      <categories>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>方便日后记录笔记，输出是对已学知识的自我总结，适合自己的知识体系才是最好的</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[统计在手，天下我有 Sample and Population​ 当我们所要调查的对象过大且调查的对象的过程中，对象一直处于变化的过程中，我们可以通过 Sample 来特征表示 Population 计算μ = sample mean X = ptopulation mean 两者计算方式无差别，只是数量的大小之分，总和/数量 如何保证样本的准确性 明确目标总体 确定抽样单位 确定抽样空间 ​ 只要是选定样本，都会存在主管和客观因素导致样本选择时发生偏倚，最终导致样本无法代表总体。偏倚的产生主要分为一下几个原因： ​ ① 抽样空间中条目不齐全，即样本包含的种类不齐全 ​ ② 抽样单位不正确，抽样单位跨度选择不合理对 ​ ③ 样本选取的抽样单位未出现在实际样本中，如调查问卷有些选项无法回复 ​ ④ 样本缺乏随机性，与第一条有些相似，最终呈现的结果即选择的样本种类不全面 常用的提高样本选取准确性的方法​ 1.3.1 简单随机抽样（闭着眼瞎抽），可分为重复抽样和不重复抽样 ​ 1.3.2 分层抽样，即划定总体的类别，在每个类别中进行随机抽样 ​ 1.3.3 整群抽样，样本中有大量相似的群组，可对群组进行随机抽样，分层抽样的另一种形式 ​ 1.3.4 系统抽样，取一定长度，再进行随机抽样 以上方法只是在一定程度上提高了准确性，但只要是随机抽样，就会存在误差。 VarianceDispersion 离中趋势的衡量，反应数值与均值之间的距离大小。 你所取得 sample 有可能不包含实际总体均值，此时Sample variance &lt; Population variance 各类方差： Random Variable可以取很多值，但是无法求解，通常用大写字母表示（与传统变量相区别）。其本身是一种函数（更像用于判断，if , elif …..else），将随机过程映射到实际数字 discrete random variable 离散随机变量，具体到某些数值 continuous random variable 连续随机变量，某些取值区间 分布离散概率分布可以将离散数据比作垫脚石，可以从一个数值跳至另一个数值，每个数值之间都有明确的间隔 几何分布：前提：事件相互独立; 每次成功失败都有可能，且概率相同; 你关心的是为了取得第一次成功需要多少次实验 公式： 即可理解为，要想第r次成功，必须先失败r-1次。随着r的逐渐增加，P（x=r）在逐渐减小。 $$期望：E(x) = ∑xP（X=x）$$通过对E(x)的计算，随着x的增大，其值越来越趋向一个固定的值：E(x) = 简单理解为单次成功的概率为p，那么每1/p次中的尝试将会有1项成功。 同理经过推到： $$几何分布的方差为：Var（x） = q/p*2$$ 二项分布：前提：独立实验，每次失败和成功的概率都相同，试验次数有限。和几何分布不同，二项分布关心的是获得成功的次数。二项分布的公式，本质是二项式的展开。我们不用通过得出概率的分布而列出每一个结果，关键在于用某种方式描述每一种结果的可能即可 公式： 根据n和p的不同, p越接近0.5，直方图越对称，p&lt;0.5时，图形向右偏斜，p&gt;0.5时，向左偏斜。 $$期望：E(x) = np$$ $$方差： Var（x）= npq$$ 伯努利分布：简单理解为二项分布的一直最简单特殊的情况 $$μ = （1-p） 0 + p 1 = p$$​ σ2 = （1-p）（0-p）2+ p（1-p）2= p（1-p） 泊松分布：前提：单独事件在给定区间内随机、独立的发生，已知该区间内的时间平均发生次数 λ X ~ Po（ λ） 公式： $$期望：E(x) = λ$$ $$方差： Var（x）= λ$$泊松分布的均值、期望和方差均为 λ，其余其他概率分布的最大不同在于，不需要进行一系列实验，只描述了事件在特定区间内的发生次数，即有稳定的 λ。如果两个时间相互独立，那么他们的 λ可以相加 例：X为电影院爆米花机一周发生故障的次数 λ1，Y为饮料机一周发生故障的次数 λ2,那么 $$P(X+Y) = P(X)+P(Y)$$ $$X+Y ~Po( λ1+ λ2)$$ 在我们计算二项分布时，当实验的次数过大，趋近于∞时，nCr可以近似的用泊松分布来代替。 X ~ B(n,p) ~ Po（λ） 此时期望：λ 近似 np ，方差 λ 近似npq ，可以导出 np近似 npq ， 那么q的值就应当近似等于1而且n很大，才可成立。 连续概率分布连续的数据往往通过测量而不是计数所得，其结果取决于精度的要求，而离散数据关心的是某一特定数值的概率，而连续数据关心的则是一个特定范围的概率。 求连续数据的概率分布通常需要先确定概率函数f(x),再通过面积来求解。 正态分布为何叫正态，因为他的f（x）看上去是合理的，符合常规的逻辑。 X ~N(μ，σ2) μ 指出曲线中央位置，σ表示分散性，其值越大，正态曲线越扁平即越宽。 公式: 在正态概率计算的过程中，可以将其标准化正态的为N(0， 1)，此可以理解为对函数进行平移和收缩（好像高一教过….），整个过程计算即为求X的标准分：$$z = （X - μ）/ σ$$X-μ 即相当于平移函数图像，除以σ，即对函数进行收缩，特定数值的标准分说明了数值与均值相距多少个标准差，可以获悉该数值与均值的相对相近程度 计算出标准分后，一般可通过概率表进行查找对应数值的概率，而一般（经验） 当区间在μ ± σ 的概率为68%， μ ± 2σ 的概率为95% ， μ ± 3σ 的概率为97.5% 中心极限定理​ 从均值为 μ 、方差为 σ2的任意一个总体中抽取样本量为n的样本，当n充分大时，样本均值的抽样分布近似服从均值为 μ、方差为 σ2/n 的正态分布。 ​ 随着样本容量 n 变大，以及抽样的次数增多，样本均值的分布越来越近似正态分布，而其标准差也会越来越小，从而使得分布函数更加收敛 置信区间​ 通过样本来预测总体，如果样本很接近整体，那么自然可以得到很好的结果，但是多近才能算“够接近”。世界如此复杂，不可能总希望用一个精确的数字来表达所有的事情，所以通过一个合理的区间来表示，可能会更合适，即可信区间。 P（a &lt; μ &lt;b）= 0.95 则（a, b）被称为置信区间 ​ 1.选择总体统计量 ​ 2.求出其抽样分布 ​ 3.决定置信水平 ​ 4.求出置信上下限 95%只是人为定义的置信区间，并不一定只有才95%范围内才可性，应当根据实际情况来选择，90% 99%，只要符合实际情况即可。 t分布：当可选取的样本容量较小，正态分布的曲线较为扁平。样本的σ2不足估计总体方差的真实值。 参数—ν，ν = n-1，ν称为自由度 ，T符合t分布且自由有度为ν： T ~ t（v） t分布有对应的概率表，根据 v 和置信区间决定系数，从而计算置信区间 假设检验​ 医药代表说XXX药能治疗90%的病人，医生抽样治疗后治愈率不到90%，这时该如何抉择，==我们假定医药代表说的是真的，对医生的抽样进行检验是否有误，最后反过来证明医药代表的话。——即假设检验== 假设检验的六个步骤： ​ ①确定要进行检验的假设 ​ 原假设：假定药物至少能治愈90%的病人,即H0:p = 0.9; ​ 备择假设：与原假设对立，如果H0不成立，即要接受H1，即H1:p = 0.9; ​ ②选择检验统计量 ​ X（测试人数）作为检验统计量，符合二项分布，X ~B （15, 0.9）​ ​ ③确定用于决策的拒绝域 ​ 确定拒绝域后，如果实际治疗的患者人数位于拒绝域以内，就可判定原假 设不成立，我们把拒绝域的分界点称为‘c’ —临界值。有些类似于置信区间，区间外的不成立，区间内的我们选择相信 ​ ==求解拒绝域==： ​ 确定下显著性水平—α，==即希望样本结果的不可能程度达到多大，就拒绝原假设==，以5%为显著性水平检验制药公司的断言，则：$$P（X &lt; c）&lt; α$$​ 显著性水平设置为多少，取决于你想以多大力度拒绝原假设，α越小，检验力度越大 ​ 在构建拒绝域时，我们应当考虑是==单尾检验==还是==双尾检验==，单尾检验的拒绝域在数据的左右任一侧，双尾的一般一份为二位于数据的两侧。==如果备选假设中只是判断&lt;或&gt;某一个条件，那么相对应的选择首或尾，如果备选假设中出现了 =，那么就需要将拒绝域一分为二，两侧各占α/2.== ​ ④求出检验统计量的p值 ​ P(X≤11） = 1- P（X &gt; 12）= 0.0555 = p，p的取值即取得样本结果或拒绝域方向的更极端的概率 ​ ⑤查看样本结果是否位于拒绝域内 ​ 此时的X取了临界拒绝域的极限值，p &gt; 0.5, 那么判定p不在拒绝域中 ​ ⑥做出决策 ​ 接受原假设。即当检验水平为5%时，我们接受医药代表的宣称。 在上述案例中，X =15仍然比较小，我们尝试增大样本容量。重新走一边判断的过程。人数定位100,其中服用有效80 ，无效20 ​ ① 原假设，备选假设皆不变。 ​ ② 样本量增加至100， X ~ B（100， 0.9），因为计算的不便，此时我们可以用正态代替二项，即X ~ N（90， 9）， ​ ③ 拒绝域： P （X &lt; -1.64) = 0.05 ​ ④ 求p ： Z = （80 -90）/3 = -3.3 P（Z &lt; -3.3） = 0.0004 ​ ⑤ 位于拒绝域内 ​ ⑥ 拒绝原假设 第一类、第二类错误​ 当进行假设检验时，可能会出现拒绝正确的原假设即为第一类错误，接受错误的备选假设即为第二类错误。 ​ 发生第一类错误的概率，即为 α， 因为处于拒绝域内，即会导致误判，这样P（第一类错误） = α ​ 发生第二类错误的概率， ​ ==先判定备选假设是否有特定的数值，如果没有无法计算，如果有才可计算==。 ​ 以上面样本容量100为例，P （Z&lt; -1.64）= 0.05 , 则说明拒绝域以外的数值Z ≥-1.64， 逆标准化，得到 X ≥ 85.05 。 ​ 对备选假设进行验证， P（X ≥ 85.08）， X ~ N（80， 16）， z = 1.27， P （Z ≥ 1.27） = 1 - （P &lt; 1.27） = 0.102. 功效检验假设的==功效==也是一种概率，原假设为假的情况下拒绝它的概率，即原假设为假接受它的相反情况，也是就第二类错误的相反情况。可得：$$功效 = 1 -β （第二类错误的概率）$$ 线性回归​ 之前所学的多是单变量自身的变化趋势，而我们往往需要发现挖掘的是多个变量之间的关系。先简单了解两个变量之间简单的线性关系。 ​ 求两个变量之间的线性关系，可以理解为：依据规定，通过一条直线模拟穿过所有的变量点，即找出最佳拟合线。$$y = a + bx$$此处的规定即为，让每个变量y与拟合线模拟出的yi值之差最小，==注意：此处不是变量y到拟合线的直线距离==。 通过最小二乘回归法计算y的出差我们可得： 我们用直线的相关系数（拟合度）， R 来表示我们最佳拟合线的精确程度，R越接近1或-1，表明拟合曲线的精确度越高： ==注：拟合曲线求出的只是两个变量之间的数学关系，并不代表两者之间一定存在某种必然联系== 代码实现： 1234567891011121314151617181920import pandas as pdfrom io import StringIOfrom sklearn import linear_modelimport matplotlib.pyplot as pltcsv_data = 'square_feet,price\n150,6450\n200,7450\n250,8450\n300,9450\n350,11450\n400,15450\n600,18450\n'df = pd.read_csv(StringIO(csv_data))#建立回归模型regr = linear_model.LinearRegression()#拟合，reshape（-1，1）表明此处拟合的为一维图像regr.fit(df['square_feet'].reshape(-1, 1), df['price'])a, b = regr.coef_, regr.intercept_#画数据散点图plt.scatter(df['square_feet'], df['price'], color='blue')#画拟合图plt.plot(df['square_feet'], regr.predict(df['square_feet'].reshape(-1,1)), color='red', linewidth=4)plt.show() 得到的结果如下图所示： χ2检验​ χ2, χ 读音为 ‘卡’，这种分布通过一个检验统计量来比较期望结果和实际结果之间的差别，然后得出观察频数极值的发生概率。主要用于1、检验拟合优度、2、检验两个变量的独立性。 ​ χ2的数值越小，观察频数和期望频数之间的总差值越小，表明当前情况很正常。但当χ2的数值大到一定程度，就表明情况异常，那么如何定义这个大的程度，即为X2分布。 χ2分布用到一个参数ν，若使用特定参数ν的X2 分布以及检验统计量X2 ，可表示为： ​ X2 ~ χ2（ν） 当ν等于1或2时， X2分布为一条先高厚底的平滑曲线，此时的观察频数更有可能接近期望频数；当ν大于2时，随着X2递增，图形先低，后高，再低，当ν很大时，图形接近正态分布。$$ν = （组数）—（限制数）$$χ2检验为单尾检验，右尾被称为拒绝域(通过χ2概率表查得)，通过检查统计量是否位于拒绝域内，即可判定根据期望分布得出的结果的可能性，若用α显著性水平进行检验，记作： ​ χ2α（ν） χ2检验的过程与假设检验一致，但多了计算期望频数和自由度以及计算检验统计量X2 χ2拟合优度检验对相对多的概率分布都有效，其难度在于自由度v的计算： 二项分布中常计为v = n-1（已知p）；v = n -2（未知p） 泊松分布中常计为v = n-1（已知λ）；v = n -2（未知λ） 正态分布常计为 v = n-1 （已知σ2&gt;和μ）; v = n - 3（未知已知σ2&gt;和μ） 参考： 深入浅出统计学 可汗：统计学 https://www.cnblogs.com/hhh5460/p/5786115.html#top]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>方便日后记录笔记，输出是对已学知识的自我总结，适合自己的知识体系才是最好的</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用hexo和coding搭建博客]]></title>
    <url>%2F2019%2F01%2F25%2F%E5%88%A9%E7%94%A8hexo%E5%92%8Ccoding%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[安装环境：​ Node.js ​ Git. 安装安装好node.js 和 git 后，桌面新建一个hexo文件夹，右键 git bash hexo 选项，输入命令行，进行hexo的安装 1npm install hexo-cli -g 并输入命令行，测试工具是否安装成功 1$ npm -v 安装命令： 1npm install hexo -g 如果安装失败尝试使用淘宝源安装： 12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install hexo-cli -g Hexo安装完成后，初始化Hexo： 1hexo init 成功后文件夹自动生成相关文件 本地调试运行，生成网页后，可在本地端口localhost:4000打开，看到hello word 1234#生成网页hexo g#本地端口打开hexo s 添加文章，在hexo\source_posts文件夹下有对应文章的名的md文件，也可以在该目录下直接新建文件。 1234#生成对应文章名的文件hexo new &quot;********&quot;#生成网页，在本地端口查看，第一篇即完成hexo s -g 部署把文章部署到github和coding上，部署哪个都可以，github国外服务器，上传后需要等一段时间才可同步，coding国内解析的速度更快 Github上新建仓库1.在 github 中创建新仓库，仓库名字格式「username.github.io」，勾选用初始化 README 2.在 Setting 页面的 Github Pages 中，Source 显示为 master branch，如果已经有域名可以在此处绑定自己的域名 绑定 SSH Key 在本地找到 id_rsa.pub 文件，进入 github，Settings →→ SSH and GPG keys →→ New SSH key，将 ssh key 粘贴进去 ssh -T git@github.com验证 ssh key 是否添加成功 配置网站hexo文件中的_config.yml 站点配置文件，即相当于软件的settings，可更改内部配置信息对网站的呈现进行更改。 1.安装主题 hexo的主题有很多，这里选取next，修改站点配置文件的themes字段为next 12#下载next主题git clone https://github.com/iissnan/hexo-theme-next themes/next 2.repository 改为自己的仓库名 123456789#repository需要使用ssh方式进行复制#注意缩进，每个冒号后面都有空格！！！！#此处同时部署coding,所以仓库又两个上传方式deploy: type: git repository: github: git@github.com:iKEMS/iKEMS.github.io.git coding: git@git.dev.tencent.com:iKEMS/iKEMS.coding.me.git branch: master 3.注册域名 提供域名的网站很多，两大云，万网等等都可以，这里选择阿里云的域名。Github setting 里 Github page 页面中填入新的域名。保存后，此时仓库中会多出 CNAME 文件，里面是域名信息。 下次执行 hexo d 时，CNAME 文件会被删掉。需要本地在 source 目录中新建 CNAME 文件，里面写上域名地址 「www.ikems.top」，下次执行 hexo d 时会自动复制到 post 文件夹下. Coding page​ coding在国内访问的速度比github要快很多，可以在部署github的同时，再部署到coding上，因为coding已经被腾讯云收购，即在腾讯云上部署，方式与上面相同。 ​ 此处需要申请SSL证书，否则会影响后面google搜索对你网站的验证，且验证时，域名的解析需要暂停对境外的解析路径，否则会一直提示失败。DNS有缓存机制，修改后需要暂停5min以上，再在coding上进行SSl申请。 域名最终解析如下： 至此此次搭建已基本完成。 next主题优化设置代码高亮主题配置文件中修改，颜色有5种可以随便选 12345# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night bright ###添加valine评论系统 添加next 5.14版本已经集成了Valine，所以无需过多配置，先注册LeanCloud账号 修改评论框样式​ 添加样式表，在themes\next\source\css\custom目录下的custom.styl文件中添加代码,传送门 重新上传后，样式如下 ==== 参考： 搭建 http://www.hotheat.top/ 主题优化 https://me.idealli.com/post/e8d13fc.html https://wuchenxu.com/2015/12/13/Static-Blog-hexo-github-7-display-updated-date/ https://www.jianshu.com/p/3a01cc514ce7?utm_source=oschina-app https://segmentfault.com/a/1190000013660164#articleHeader15 valine评论系统： https://blog.csdn.net/blue_zy/article/details/79071414]]></content>
      <categories>
        <category>使用教程</category>
      </categories>
      <tags>
        <tag>方便日后记录笔记，输出是对已学知识的自我总结，适合自己的知识体系才是最好的</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习]]></title>
    <url>%2F2019%2F01%2F25%2FMySQL%2F</url>
    <content type="text"><![CDATA[SQL 数据库基础数据库（database）​ 一个以某种有das组织的方式存储的数据集合，此处的数据集合通常一个文件或是一组文件。 ​ 区别： ​ 数据库和DBMS并不指同一个东西，DBMS是我们操作数据库的软件，我们平时接触的基本都是DBMS。而数据库只负责存储数据，至于它在什么地方，以什么形式存在，暂且可以不用关心。 表（table）​ 对于一个数据库，我们可以将其进行分类，近似的可以理解为对电脑的磁盘进行分区，C盘，D盘……等等。而当我们往其中存储数据的时候，我们可以存储在不同盘符的表中，表是对数据的进一步分类存储单元。 ​ 在同一级的数据库中，表的名称应当唯一，但在不同的数据库中，可以相同。 行列​ 表中的行列用于存储具体的数据信息，可以简单 粗暴的理解为excle中的行列，虽然不完全一致，但是意思其实差不多。 主键​ 唯一标识表中每行的这个列（或这几列）称为主键。主键用来表示一个特定的行。主键并非强制标识，但为了以后的操作和管理，应该养成设置主键的习惯。 任意两行都不具有相同的主键值； 每一行都必须具有一个主键值（主键列不允许NULL值）； 主键列中的值不允许修改或更新； 主键值不能重用（如果某行从表中删除，它的主键不能赋给以后的新行） 通常以身份证信息作为数据存储的主键，在我天朝而言，每个人的身份证号码都独一无二，这也是现在很多场景需要录入用户身份信息的原因。 SQL​ 结构化查询语言（Structured Query Language），是专门与数据库进行沟通的语言，SQL有自我的标准，但不同的数据库可能有些扩展语法，导致各不相同，但理解了查询的逻辑，基本大同小异。 ​ 数据库操作四大流氓操作，学会了就可以在简历里不要脸的写熟练掌握Mysql了： 查询 插入 删除 修改 检索普通检索关键词：select，它的用途是从一个或多个表中检索信息。其核心逻辑为：从什么地方搜索出你想的什么东西 1SELECT prod_name FROM Products; 以上的意思即是从Products中搜索出prod_name列的全部数据,当检索多个列时，**需要给出每个列的列名，以逗号分隔**。 1SELECT prod_id, prod_name, prod_price FROM Products; 1SELECT * FROM Products; *通常作为通配符，可以返回表中所有列。 1SELECT DISTINCT vend_id FROM Products; DISTINCT关键字,,可以让数据库返回每列中不同的值。 1SELECT prod_name FROM Products LIMIT 5 OFFSET 5; LIMIT关键字，限制筛选出的数据行数，OFFSET关键字指让数据库从第五行开始筛选，即从何处筛选，筛选几行。OFFSET 1 会检索第2行，而不是第一行，因为第一个被检索的是第0行。 每个数据库中的限制语句不完全一致，如果不能移植，需要在具体库中进行修改，这个以后有时间再补充。 123456SELECT prod_name -- 这是一条注释FROM Products;/* SELECT prod_name, vend_idFROM Products; */SELECT prod_nameFROM Products; – 表示进行行内注释, /…… / 符号中的部分也会被注释掉。 排序检索​ 关系数据库设计理论认为，如果不明确规定排序顺序，则不应该假定检索出的数据的顺序有任何意义。 1SELECT prod_name FROM Products ORDER BY prod_name; ​ 关键字ORDER BY 指定DBMS对prod_name列以字母顺序排序，指定一条ORDER BY子句时，应该保证它是 SELECT语句中最后一条子句。如果它不是最后的子句，将会出现错误消息。 如果你想显示A列，但ORDER BY B列，也完全合法。 123SELECT prod_id, prod_price, prod_nameFROM ProductsORDER BY prod_price, prod_name; ​ 重要的是理解在按多个列排序时，排序的顺序完全按规定进行。换句话说，对于上述例子中的输出，仅在多个行具有相同的 prod_price值时才对产品按prod_name进行排序。如果prod_price列中所有的值都是唯一的，则不会按prod_name排序。 ​ 当列名后面加上 DESC关键词时，表示倒序排序。在多个列上降序排序，如果想在多个列上进行降序排序，必须对每一列指定DESC关键字，相对的升序是ASC，不过一般都是默认升序，写不写都行。 ​ 存疑：如果多文本性数据进行排序，A与a是否相同，a和Z谁在前，这一般取决于数据的设置方式，如果有必要可以连与管理员协助进行更改。 过滤数据​ 检索所需数据需要指定搜索条件（search criteria），搜索条件也称为过滤条件（filter condition），==WHERE==子句在表名（FROM子句）之后给出： 123SELECT prod_name, prod_priceFROM ProductsWHERE prod_price = 3.49 ​ 在同时使用 ORDER BY和 WHERE子句时，应该让 ORDER BY位于 WHERE之后，否则将会产生错误。 123SELECT vend_id, prod_nameFROM ProductsWHERE vend_id &lt;&gt; &apos;DLL01&apos;; ​ ‘DLL01’ 单引号用来限定字符串。如果将值与字符串类型的列进行比较，就需要限定引号。用来与数值列进行比较的值不用引号. 123SELECT prod_name, prod_priceFROM ProductsWHERE prod_price BETWEEN 5 AND 10; ​ 在使用BETWEEN时，必须指定两个值——所需范围的低端值和高端值。这两个值必须用AND关键字分隔. 123SELECT prod_id, prod_price, prod_nameFROM ProductsWHERE vend_id = &apos;DLL01&apos; AND prod_price &lt;= 4; ​ 可以增加多个过滤条件，每个条件间都要使用AND, OR,关键字. 1234SELECT prod_name, prod_priceFROM ProductsWHERE (vend_id = &apos;DLL01&apos; OR vend_id = &apos;BRS01&apos;)AND prod_price &gt;= 10; ​ AND 处理的优先级要高于 OR，因此，在进行条件筛选时，需要注意使用括号理清筛选的逻辑。 1234SELECT prod_name, prod_priceFROM ProductsWHERE vend_id IN ( &apos;DLL01&apos;, &apos;BRS01&apos; )ORDER BY prod_name; ​ IN 操作符输出的结果与OR相似，其就等于 vend_id = ‘DLL01’ OR vend_id = ‘BRS01’。IN 操作符后跟由逗号分隔的合法值，这些值必须括在圆括号中。 使用 IN 的优点： 在有很多合法选项时，IN操作符的语法更清楚，更直观 在与其他AND和OR操作符组合使用IN时，求值顺序更容易管理 IN操作符一般比一组OR操作符执行得更快（在上面这个合法选项很少的例子中，你看不出性能差异）。 IN的最大优点是可以包含其他 SELECT语句，能够更动态地建立WHERE子句 1234SELECT prod_nameFROM ProductsWHERE NOT vend_id = &apos;DLL01&apos;ORDER BY prod_name; ​ NOT 否定跟在其后的条件,也可以使用&lt;&gt;操作符来完成,NOT从不单独使用（它总是与其他操作符一起使用)，在与 IN 操作符联合使用时，NOT 可以非常简单地找出与条件列表不匹配的行，大多数DBMS中允许 NOT 否定任何条件 通配符检索​ 利用通配符（wildcard），可以创建比较特定数据的搜索模式，用来匹配值的一部分的特殊字符。为在搜索子句中使用通配符，必须使用LIKE操作符。LIKE指示 DBMS，后跟的搜索模式利用通配符匹配而不是简单的相等匹配进行比较。通配符搜索只能用于文本字段（字符串），非文本数据类型字段不能使用通配符搜索。 123SELECT prod_id, prod_nameFROM ProductsWHERE prod_name LIKE &apos;Fish%&apos;; ​ 此句的意思是匹配所有Fish开头的字符，百分号（%）是最常用的通配符，通常可以匹配0、1或多个字符，但不可以匹配NULL。搜索模式’%bean bag%’表示匹配任何位置上包含文本bean bag的值，不论它之前或之后出现什么字符。 123SELECT prod_id, prod_nameFROM ProductsWHERE prod_name LIKE &apos;__ inch teddy bear&apos;; ​ 下划线（_）的用途与%一样，但它只匹配单个字符，而不是多个字符。 __只能匹配两个字符，例如上述例子中，它可以匹配两个字符的12 、18，但是不可以匹配8， 注意： 不要过度使用通配符。如果其他操作符能达到相同的目的，应该使用其他操作符。 在确实需要使用通配符时，也尽量不要把它们用在搜索模式的开始处。把通配符置于开始处，搜索起来是最慢的。 汇总数据聚集函数​ 数据库的数据多尔杂，在大多数情况下，我们想要的只是特定数据（行，列）的集合。SQL的聚集函数（aggregate function）则可以很好的实现这一功能。常用的聚集函数有以下几种： AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 123SELECT AVG(prod_price) AS avg_priceFROM ProductsWHERE vend_id = &apos;DLL01&apos;; ​ 此句中求出prod_price的平均值 以avg_price进行输出。AVG()只能用来确定特定数值列的平均值，而且列名必须作为函数参数给出。为了获得多个列的平均值，必须使用多个AVG()函数。AVG()函数忽略列值为NULL的行。 12SELECT COUNT(*) AS num_custFROM Customers; ​ 如果指定列名，则COUNT()函数会忽略指定列的值为空的行，但如果COUNT()函数中用的是星号（*），则不忽略 12SELECT MAX(prod_price) AS max_priceFROM Products; ​ 虽然 MAX()一般用来找出最大的数值或日期值，但许多（并非所有）DBMS允许将它用来返回任意列中的最大值，包括返回文本列中的最大值。在用于文本数据时，MAX()返回按该列排序后的最后一行。MAX()函数忽略列值为NULL的行。 12SELECT MIN(prod_price) AS min_priceFROM Products; ​ 虽然 MIN()一般用来找出最小的数值或日期值，但许多（并非所有）DBMS允许将它用来返回任意列中的最小值，包括返回文本列中的最小值。在用于文本数据时，MIN()返回该列排序后最前面的行。MIN()函数忽略列值为NULL的行。 123SELECT SUM(item_price*quantity) AS total_priceFROM OrderItemsWHERE order_num = 20005; ​ SUM 函数可以进行多列计算，其他几个聚集函数亦可如此。SUM()函数忽略列值为NULL的行。 聚集不同的数据123SELECT AVG(DISTINCT prod_price) AS avg_priceFROM ProductsWHERE vend_id = &apos;DLL01&apos;; ​ 次语句是讲prod_price列中去重后进行计算，如果指定列名，则 DISTINCT只能用于 COUNT()。DISTINCT不能用于COUNT(*)。类似地，DISTINCT必须使用列名，不能用于计算或表达式。ALL是默认参数，如果不注明DISTNCT，就默认为ALL。 12345SELECT COUNT(*) AS num_items,MIN(prod_price) AS price_min,MAX(prod_price) AS price_max,AVG(prod_price) AS price_avgFROM Products; ​ 函数可以联合使用，在指定别名以包含某个聚集函数的结果时，不应该使用表中实际的列名，以免DBMS报错。 分组数据123SELECT vend_id, COUNT(*) AS num_prodsFROM ProductsGROUP BY vend_id; ​ 此句实现的功能是将 vend_id分类计数，并按照此列进行排列。GROUP BY子句必须出现在WHERE子句之后，ORDER BY子句之前。如果分组列中包含具有NULL值的行，则NULL将作为一个分组返回。如果列中有多行NULL值，它们将分为一组。 ​ 当你想排除一些分组时，WHERE不可用，因为其针对的是行而不是分组，WHERE没有分组的概念。HAVING非常类似于WHERE。事实上，目前为止所学过的所有类型的WHERE子句都可以用HAVING来替代。唯一的差别是，WHERE过滤行，而HAVING过滤分组。 1234SELECT cust_id, COUNT(*) AS ordersFROM OrdersGROUP BY cust_idHAVING COUNT(*) &gt;= 2; ​ 由此可以看出，WHERE在数据分组前进行过滤，HAVING在数据分组后进行过滤。这是一个重要的区别，WHERE排除的行不包括在分组中。使用 HAVING时应该结合GROUP BY子句，而WHERE子句用于标准的行级过滤。 区分GROUP BY和ORDER BY： ORDER BY GROUP BY 对产生的输出排 对行分组，但输出可能不是分组的顺序 任意列都可以使用（甚至非 选择的列也可以使用） | 只可能使用选择列或表达式列，而且必须使用每个选择列表达式 || 不一定需 | 如果与聚集函数一起使用列（或表达式），则必须使用 | ​ 一般在使用 GROUP BY子句时，应该也给出 ORDER BY子句。这是保证数据正确排序的唯一方法。千万不要仅依赖GROUP BY排序数据。 12345SELECT order_num, COUNT(*) AS itemsFROM OrderItemsGROUP BY order_numHAVING COUNT(*) &gt;= 3ORDER BY items, order_num; ​ 此语句对order_num进行分类和过滤后，又按照items进行排序，如果不加ORDER BY 则会按照order_num进行排序输出。 SELECT子句顺序： 子句 说明 是否必须使用 FROM 从中检索数据的表 仅在列表选择数据时使用 WHER 行级过滤 否 GROUP BY 分组说明 仅在按组计算聚集时使用 HAVING 组级过滤 否 ORDER BY 输出排序顺序 否 子查询​ SQL还允许创建子查询（subquery），即嵌套在其他查询中的查询。 1234567SELECT cust_name, cust_contactFROM CustomersWHERE cust_id IN (SELECT cust_id FROM Orders WHERE order_num IN (SELECT order_num FROM OrderItems WHERE prod_id = &apos;RGAN01&apos;)); ​ 此句中一共需要执行三个SELECT查询，最里边的子查询返回订单号列表，此列表用于其外面的子查询的 WHERE子句。外面的子查询返回顾客 ID列表，此顾客 ID列表用于最外层查询的WHERE子句。最外层查询返回所需的数据。 ​ 作为子查询的 SELECT语句只能查询单个列。企图检索多个列将返回错误 1234567SELECT cust_name, cust_state, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS ordersFROM CustomersORDER BY cust_name; ​ 在使用SELECT语句作为子查询是，对于限定条件WHERE等，应该使用完全列名，即对应为表中的对应列，而不该简单使用列名，这样会是的数据误解语句的含义，对列的自身做比较。 联结表内联结​ SQL最强大的功能之一就是能在数据查询的执行中联结（join）表。其功能可以理解为通过语句把散落在各个表中的数据按照一定关系呈现出来。 ​ 相同的数据出现多次决不是一件好事，这是关系数据库设计的基础。关系表的设计就是要把信息分解成多个表，一类数据一个表。各表通过某些共同的值互相关联（所以才叫关系数据库）。 这样做的好处是： 信息不重复，不会浪费时间和空间； 如果客户信息变动，可以只更新存储信息的表中的数据，其他相关表中的数据可以不动 由于数据不重复，使得处理数据和生成报表更简答 这样存储使得关系型数据库的伸缩性比非关系数据要好。 可伸缩性：能够适应不断增加的工作量而不失败， 123SELECT vend_name, prod_name, prod_priceFROM Vendors, ProductsWHERE Vendors.vend_id = Products.vend_id; ​ 此处应该明确列名，事实上为了数据检索的准确性，牺牲一些美观性，保持表明和列名的完整性十分必要。 123SELECT vend_name, prod_name, prod_priceFROM Vendors INNER JOIN ProductsON Vendors.vend_id = Products.vend_id; ​ 此句与上句返回的结果相同，只是FROM字句表达的语法不同，具体用哪种可以根据个人的习惯。 ​ 虽然 SQL本身不限制每个联结约束中表的数目，但实际上许多 DBMS都有限制。这种处理可能非常耗费资源，因此应该注意，不要联结不必要的表。联结的表越多，性能下降越厉害。 12345SELECT cust_name, cust_contactFROM Customers AS C, Orders AS O, OrderItems AS OIWHERE C.cust_id = O.cust_idAND OI.order_num = O.order_numAND prod_id = &apos;RGAN01&apos;; ​ 表别名的使用适用于表名过长，以及一次语句中多次使用该表，使得语句简洁美观更易阅读 自联结12345678910SELECT cust_id, cust_name, cust_contactFROM CustomersWHERE cust_name = (SELECT cust_name FROM Customers WHERE cust_contact = &apos;Jim Jones&apos;);SELECT c1.cust_id, c1.cust_name, c1.cust_contactFROM Customers AS c1, Customers AS c2WHERE c1.cust_name = c2.cust_nameAND c2.cust_contact = &apos;Jim Jones&apos;; ​ 第一个语句使用了子查询，第二个使用了自联结。Customers第一次出现用了别名 C1，第二次出现用了别名C2。现在可以将这些别名用作表名。例如，SELECT语句使用C1前缀明确给出所需列的全名。如果不这样，DBMS将返回错误，因为名为 cust_id、cust_name、cust_contact的列各有两个。WHERE首先联结两个表，然后按第二个表中的cust_contact过滤数据，返回所需的数据。 ​ 自联结通常作为外部语句，用来替代从相同表中检索数据的使用子查询语句。虽然最终的结果是相同的，但许多 DBMS处理联结远比处理子查询快得多。 自然联结 写在最后： SQL语句中通常都不区分大小写，但是无论是美观还是为了逻辑清晰，我们应当养成自己的书写和语法习惯，这点非常重要！！！ 每条完整的SQL语句都应该加上分号. 操作符作为谓词时并非为操作符，虽然最终的结果相同，例如 LIKE ​]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>熟练掌握SQL语句</tag>
      </tags>
  </entry>
</search>
